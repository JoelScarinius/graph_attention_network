{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f6f9c6",
   "metadata": {},
   "source": [
    "\n",
    "Title: Graph attention network (GAT) for node classification\n",
    "Author: [akensert](https://github.com/akensert)\n",
    "Date created: 2021/09/13\n",
    "Last modified: 2021/12/26\n",
    "Description: An implementation of a Graph Attention Network (GAT) for node classification.\n",
    "Accelerator: GPU\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "[Graph neural networks](https://en.wikipedia.org/wiki/Graph_neural_network)\n",
    "is the preferred neural network architecture for processing data structured as\n",
    "graphs (for example, social networks or molecule structures), yielding\n",
    "better results than fully-connected networks or convolutional networks.\n",
    "\n",
    "In this tutorial, we will implement a specific graph neural network known as a\n",
    "[Graph Attention Network](https://arxiv.org/abs/1710.10903) (GAT) to predict labels of\n",
    "scientific papers based on what type of papers cite them (using the\n",
    "[Cora](https://linqs.soe.ucsc.edu/data) dataset).\n",
    "\n",
    "### References\n",
    "\n",
    "For more information on GAT, see the original paper\n",
    "[Graph Attention Networks](https://arxiv.org/abs/1710.10903) as well as\n",
    "[DGL's Graph Attention Networks](https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html)\n",
    "documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c74d85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Import packages\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 6)\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547fa03",
   "metadata": {},
   "source": [
    "\n",
    "## Obtain the dataset\n",
    "\n",
    "The preparation of the [Cora dataset](https://linqs.soe.ucsc.edu/data) follows that of the\n",
    "[Node classification with Graph Neural Networks](https://keras.io/examples/graph/gnn_citations/)\n",
    "tutorial. Refer to this tutorial for more details on the dataset and exploratory data analysis.\n",
    "In brief, the Cora dataset consists of two files: `cora.cites` which contains *directed links* (citations) between\n",
    "papers; and `cora.content` which contains *features* of the corresponding papers and one\n",
    "of seven labels (the *subject* of the paper).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6d574b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = keras.utils.get_file(\n",
    "    fname=\"cora.tgz\",\n",
    "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
    "    extract=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6fe3bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joel/.keras/datasets/cora_extracted/cora'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join(os.path.dirname(zip_file), \"cora_extracted/cora\")\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da01b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.cites\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "papers = pd.read_csv(\n",
    "    os.path.join(data_dir, \"cora.content\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82b7619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      target  source\n",
      "0          0      21\n",
      "1          0     905\n",
      "2          0     906\n",
      "...      ...     ...\n",
      "5426    1874    2586\n",
      "5427    1876    1874\n",
      "5428    1897    2707\n",
      "\n",
      "[5429 rows x 2 columns]\n",
      "      paper_id  term_0  term_1  ...  term_1431  term_1432  subject\n",
      "0          462       0       0  ...          0          0        2\n",
      "1         1911       0       0  ...          0          0        5\n",
      "2         2002       0       0  ...          0          0        4\n",
      "...        ...     ...     ...  ...        ...        ...      ...\n",
      "2705      2372       0       0  ...          0          0        1\n",
      "2706       955       0       0  ...          0          0        0\n",
      "2707       376       0       0  ...          0          0        2\n",
      "\n",
      "[2708 rows x 1435 columns]\n",
      "Edges shape:\t\t (5429, 2)\n",
      "Node features shape: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "\n",
    "print(citations)\n",
    "\n",
    "print(papers)\n",
    "\n",
    "\n",
    "### Split the dataset\n",
    "\n",
    "\n",
    "# Obtain random indices\n",
    "random_indices = np.random.permutation(range(papers.shape[0]))\n",
    "\n",
    "# 50/50 split\n",
    "train_data = papers.iloc[random_indices[: len(random_indices) // 2]]\n",
    "test_data = papers.iloc[random_indices[len(random_indices) // 2 :]]\n",
    "\n",
    "\n",
    "### Prepare the graph data\n",
    "\n",
    "\n",
    "# Obtain paper indices which will be used to gather node states\n",
    "# from the graph later on when training the model\n",
    "train_indices = train_data[\"paper_id\"].to_numpy()\n",
    "test_indices = test_data[\"paper_id\"].to_numpy()\n",
    "\n",
    "# Obtain ground truth labels corresponding to each paper_id\n",
    "train_labels = train_data[\"subject\"].to_numpy()\n",
    "test_labels = test_data[\"subject\"].to_numpy()\n",
    "\n",
    "# Define graph, namely an edge tensor and a node feature tensor\n",
    "edges = tf.convert_to_tensor(citations[[\"target\", \"source\"]])\n",
    "node_states = tf.convert_to_tensor(papers.sort_values(\"paper_id\").iloc[:, 1:-1])\n",
    "\n",
    "# Print shapes of the graph\n",
    "print(\"Edges shape:\\t\\t\", edges.shape)\n",
    "print(\"Node features shape:\", node_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c0b30",
   "metadata": {},
   "source": [
    "\n",
    "## Build the model\n",
    "\n",
    "GAT takes as input a graph (namely an edge tensor and a node feature tensor) and\n",
    "outputs \\[updated\\] node states. The node states are, for each target node, neighborhood\n",
    "aggregated information of *N*-hops (where *N* is decided by the number of layers of the\n",
    "GAT). Importantly, in contrast to the\n",
    "[graph convolutional network](https://arxiv.org/abs/1609.02907) (GCN)\n",
    "the GAT makes use of attention mechanisms\n",
    "to aggregate information from neighboring nodes (or *source nodes*). In other words, instead of simply\n",
    "averaging/summing node states from source nodes (*source papers*) to the target node (*target papers*),\n",
    "GAT first applies normalized attention scores to each source node state and then sums.\n",
    "\n",
    "\n",
    "\n",
    "### (Multi-head) graph attention layer\n",
    "\n",
    "The GAT model implements multi-head graph attention layers. The `MultiHeadGraphAttention`\n",
    "layer is simply a concatenation (or averaging) of multiple graph attention layers\n",
    "(`GraphAttention`), each with separate learnable weights `W`. The `GraphAttention` layer\n",
    "does the following:\n",
    "\n",
    "Consider inputs node states `h^{l}` which are linearly transformed by `W^{l}`, resulting in `z^{l}`.\n",
    "\n",
    "For each target node:\n",
    "\n",
    "1. Computes pair-wise attention scores `a^{l}^{T}(z^{l}_{i}||z^{l}_{j})` for all `j`,\n",
    "resulting in `e_{ij}` (for all `j`).\n",
    "`||` denotes a concatenation, `_{i}` corresponds to the target node, and `_{j}`\n",
    "corresponds to a given 1-hop neighbor/source node.\n",
    "2. Normalizes `e_{ij}` via softmax, so as the sum of incoming edges' attention scores\n",
    "to the target node (`sum_{k}{e_{norm}_{ik}}`) will add up to 1.\n",
    "3. Applies attention scores `e_{norm}_{ij}` to `z_{j}`\n",
    "and adds it to the new target node state `h^{l+1}_{i}`, for all `j`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "\n",
    "        # Linearly transform node states\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        # (1) Compute pair-wise attention scores\n",
    "        node_states_expanded = tf.gather(node_states_transformed, edges)\n",
    "        node_states_expanded = tf.reshape(node_states_expanded, (tf.shape(edges)[0], -1))\n",
    "        attention_scores = tf.nn.leaky_relu(tf.matmul(node_states_expanded, self.kernel_attention))\n",
    "        attention_scores = tf.squeeze(attention_scores, -1)\n",
    "\n",
    "        # (2) Normalize attention scores\n",
    "        attention_scores = tf.math.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        attention_scores_sum = tf.math.unsorted_segment_sum(\n",
    "            data=attention_scores,\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.reduce_max(edges[:, 0]) + 1,\n",
    "        )\n",
    "        attention_scores_sum = tf.repeat(attention_scores_sum, tf.math.bincount(tf.cast(edges[:, 0], \"int32\")))\n",
    "        attention_scores_norm = attention_scores / attention_scores_sum\n",
    "\n",
    "        # (3) Gather node states of neighbors, apply attention scores and aggregate\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * attention_scores_norm[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=tf.shape(node_states)[0],\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, pair_indices = inputs\n",
    "\n",
    "        # Obtain outputs from each attention head\n",
    "        outputs = [attention_layer([atom_features, pair_indices]) for attention_layer in self.attention_layers]\n",
    "        # Concatenate or average the node states from each head\n",
    "        if self.merge_type == \"concat\":\n",
    "            outputs = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            outputs = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        # Activate and return node states\n",
    "        return tf.nn.relu(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0fb75",
   "metadata": {},
   "source": [
    "\n",
    "### Implement training logic with custom `train_step`, `test_step`, and `predict_step` methods\n",
    "\n",
    "Notice, the GAT model operates on the entire graph (namely, `node_states` and\n",
    "`edges`) in all phases (training, validation and testing). Hence, `node_states` and\n",
    "`edges` are passed to the constructor of the `keras.Model` and used as attributes.\n",
    "The difference between the phases are the indices (and labels), which gathers\n",
    "certain outputs (`tf.gather(outputs, indices)`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4821cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_states,\n",
    "        edges,\n",
    "        hidden_units,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        output_dim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_states = node_states\n",
    "        self.edges = edges\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        x = self.preprocess(node_states)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer([x, edges]) + x\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        indices, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            outputs = self([self.node_states, self.edges])\n",
    "            # Compute loss\n",
    "            loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        # Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        indices = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute probabilities\n",
    "        return tf.nn.softmax(tf.gather(outputs, indices))\n",
    "\n",
    "    def test_step(self, data):\n",
    "        indices, labels = data\n",
    "        # Forward pass\n",
    "        outputs = self([self.node_states, self.edges])\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(labels, tf.gather(outputs, indices))\n",
    "        # Update metric(s)\n",
    "        self.compiled_metrics.update_state(labels, tf.gather(outputs, indices))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b74ae47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 - 13s - 3s/step - acc: 0.3529 - loss: -2.7733e-02 - val_loss: -1.5518e-02\n",
      "Epoch 2/100\n",
      "5/5 - 1s - 242ms/step - acc: 0.6838 - loss: 0.0052 - val_loss: 0.0128\n",
      "Epoch 3/100\n",
      "5/5 - 1s - 229ms/step - acc: 0.8015 - loss: 0.0222 - val_loss: 0.0070\n",
      "Epoch 4/100\n",
      "5/5 - 1s - 238ms/step - acc: 0.7794 - loss: 0.0226 - val_loss: 0.0119\n",
      "Epoch 5/100\n",
      "5/5 - 1s - 233ms/step - acc: 0.7721 - loss: 0.0252 - val_loss: 0.0106\n",
      "Epoch 6/100\n",
      "5/5 - 1s - 225ms/step - acc: 0.8015 - loss: 0.0190 - val_loss: 0.0067\n",
      "Epoch 7/100\n",
      "5/5 - 1s - 232ms/step - acc: 0.8015 - loss: 0.0213 - val_loss: 0.0120\n",
      "Epoch 8/100\n",
      "5/5 - 1s - 227ms/step - acc: 0.8162 - loss: 0.0205 - val_loss: 0.0087\n",
      "Epoch 9/100\n",
      "5/5 - 1s - 276ms/step - acc: 0.8162 - loss: 0.0183 - val_loss: 0.0108\n",
      "Epoch 10/100\n",
      "5/5 - 2s - 333ms/step - acc: 0.8015 - loss: 0.0210 - val_loss: 0.0142\n",
      "Epoch 11/100\n",
      "5/5 - 2s - 320ms/step - acc: 0.8162 - loss: 0.0242 - val_loss: 0.0167\n",
      "Epoch 12/100\n",
      "5/5 - 2s - 331ms/step - acc: 0.8309 - loss: 0.0257 - val_loss: 0.0173\n",
      "Epoch 13/100\n",
      "5/5 - 2s - 349ms/step - acc: 0.8309 - loss: 0.0259 - val_loss: 0.0173\n",
      "Epoch 14/100\n",
      "5/5 - 2s - 316ms/step - acc: 0.8235 - loss: 0.0254 - val_loss: 0.0164\n",
      "Epoch 15/100\n",
      "5/5 - 2s - 305ms/step - acc: 0.8162 - loss: 0.0246 - val_loss: 0.0159\n",
      "Epoch 16/100\n",
      "5/5 - 2s - 305ms/step - acc: 0.8015 - loss: 0.0240 - val_loss: 0.0153\n",
      "Epoch 17/100\n",
      "5/5 - 2s - 304ms/step - acc: 0.8015 - loss: 0.0236 - val_loss: 0.0153\n",
      "Epoch 18/100\n",
      "5/5 - 2s - 306ms/step - acc: 0.8015 - loss: 0.0236 - val_loss: 0.0155\n",
      "Epoch 19/100\n",
      "5/5 - 2s - 318ms/step - acc: 0.8015 - loss: 0.0239 - val_loss: 0.0158\n",
      "Epoch 20/100\n",
      "5/5 - 2s - 349ms/step - acc: 0.8088 - loss: 0.0241 - val_loss: 0.0161\n",
      "Epoch 21/100\n",
      "5/5 - 2s - 340ms/step - acc: 0.8162 - loss: 0.0244 - val_loss: 0.0164\n",
      "Epoch 22/100\n",
      "5/5 - 2s - 331ms/step - acc: 0.8162 - loss: 0.0247 - val_loss: 0.0168\n",
      "Epoch 23/100\n",
      "5/5 - 2s - 320ms/step - acc: 0.8162 - loss: 0.0251 - val_loss: 0.0170\n",
      "Epoch 24/100\n",
      "5/5 - 2s - 317ms/step - acc: 0.8162 - loss: 0.0253 - val_loss: 0.0173\n",
      "Epoch 25/100\n",
      "5/5 - 2s - 317ms/step - acc: 0.8162 - loss: 0.0256 - val_loss: 0.0174\n",
      "Epoch 26/100\n",
      "5/5 - 2s - 329ms/step - acc: 0.8162 - loss: 0.0256 - val_loss: 0.0175\n",
      "Epoch 27/100\n",
      "5/5 - 2s - 303ms/step - acc: 0.8162 - loss: 0.0256 - val_loss: 0.0175\n",
      "Epoch 28/100\n",
      "5/5 - 2s - 307ms/step - acc: 0.8162 - loss: 0.0257 - val_loss: 0.0177\n",
      "Epoch 29/100\n",
      "5/5 - 2s - 308ms/step - acc: 0.8162 - loss: 0.0258 - val_loss: 0.0178\n",
      "Epoch 30/100\n",
      "5/5 - 2s - 339ms/step - acc: 0.8162 - loss: 0.0260 - val_loss: 0.0180\n",
      "Epoch 31/100\n",
      "5/5 - 2s - 329ms/step - acc: 0.8162 - loss: 0.0262 - val_loss: 0.0182\n",
      "Epoch 32/100\n",
      "5/5 - 2s - 318ms/step - acc: 0.8162 - loss: 0.0264 - val_loss: 0.0183\n",
      "Epoch 33/100\n",
      "5/5 - 2s - 316ms/step - acc: 0.8162 - loss: 0.0265 - val_loss: 0.0184\n",
      "Epoch 34/100\n",
      "5/5 - 2s - 318ms/step - acc: 0.8162 - loss: 0.0265 - val_loss: 0.0184\n",
      "Epoch 35/100\n",
      "5/5 - 2s - 365ms/step - acc: 0.8162 - loss: 0.0266 - val_loss: 0.0186\n",
      "Epoch 36/100\n",
      "5/5 - 2s - 330ms/step - acc: 0.8162 - loss: 0.0268 - val_loss: 0.0187\n",
      "Epoch 37/100\n",
      "5/5 - 2s - 337ms/step - acc: 0.8162 - loss: 0.0268 - val_loss: 0.0187\n",
      "Epoch 38/100\n",
      "5/5 - 2s - 323ms/step - acc: 0.8162 - loss: 0.0269 - val_loss: 0.0188\n",
      "Epoch 39/100\n",
      "5/5 - 2s - 358ms/step - acc: 0.8162 - loss: 0.0269 - val_loss: 0.0188\n",
      "Epoch 40/100\n",
      "5/5 - 2s - 374ms/step - acc: 0.8162 - loss: 0.0269 - val_loss: 0.0189\n",
      "Epoch 41/100\n",
      "5/5 - 2s - 335ms/step - acc: 0.8162 - loss: 0.0270 - val_loss: 0.0189\n",
      "Epoch 42/100\n",
      "5/5 - 2s - 315ms/step - acc: 0.8162 - loss: 0.0271 - val_loss: 0.0190\n",
      "Epoch 43/100\n",
      "5/5 - 2s - 366ms/step - acc: 0.8162 - loss: 0.0271 - val_loss: 0.0191\n",
      "Epoch 44/100\n",
      "5/5 - 2s - 354ms/step - acc: 0.8162 - loss: 0.0272 - val_loss: 0.0191\n",
      "Epoch 45/100\n",
      "5/5 - 2s - 407ms/step - acc: 0.8162 - loss: 0.0272 - val_loss: 0.0192\n",
      "Epoch 46/100\n",
      "5/5 - 2s - 345ms/step - acc: 0.8162 - loss: 0.0273 - val_loss: 0.0193\n",
      "Epoch 47/100\n",
      "5/5 - 2s - 348ms/step - acc: 0.8162 - loss: 0.0274 - val_loss: 0.0194\n",
      "Epoch 48/100\n",
      "5/5 - 2s - 319ms/step - acc: 0.8162 - loss: 0.0276 - val_loss: 0.0195\n",
      "Epoch 49/100\n",
      "5/5 - 2s - 339ms/step - acc: 0.8162 - loss: 0.0277 - val_loss: 0.0196\n",
      "Epoch 50/100\n",
      "5/5 - 2s - 313ms/step - acc: 0.8162 - loss: 0.0278 - val_loss: 0.0197\n",
      "Epoch 51/100\n",
      "5/5 - 2s - 314ms/step - acc: 0.8162 - loss: 0.0278 - val_loss: 0.0197\n",
      "Epoch 52/100\n",
      "5/5 - 2s - 302ms/step - acc: 0.8162 - loss: 0.0279 - val_loss: 0.0198\n",
      "Epoch 53/100\n",
      "5/5 - 2s - 342ms/step - acc: 0.8162 - loss: 0.0279 - val_loss: 0.0198\n",
      "Epoch 54/100\n",
      "5/5 - 2s - 423ms/step - acc: 0.8162 - loss: 0.0279 - val_loss: 0.0198\n",
      "Epoch 55/100\n",
      "5/5 - 2s - 379ms/step - acc: 0.8162 - loss: 0.0279 - val_loss: 0.0199\n",
      "Epoch 56/100\n",
      "5/5 - 2s - 355ms/step - acc: 0.8162 - loss: 0.0280 - val_loss: 0.0198\n",
      "Epoch 57/100\n",
      "5/5 - 2s - 347ms/step - acc: 0.8162 - loss: 0.0280 - val_loss: 0.0199\n",
      "Epoch 58/100\n",
      "5/5 - 2s - 345ms/step - acc: 0.8162 - loss: 0.0281 - val_loss: 0.0200\n",
      "Epoch 59/100\n",
      "5/5 - 2s - 344ms/step - acc: 0.8162 - loss: 0.0281 - val_loss: 0.0201\n",
      "Epoch 60/100\n",
      "5/5 - 2s - 316ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0200\n",
      "Epoch 61/100\n",
      "5/5 - 2s - 321ms/step - acc: 0.8162 - loss: 0.0281 - val_loss: 0.0200\n",
      "Epoch 62/100\n",
      "5/5 - 2s - 306ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 63/100\n",
      "5/5 - 2s - 335ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 64/100\n",
      "5/5 - 2s - 301ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 65/100\n",
      "5/5 - 2s - 328ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 66/100\n",
      "5/5 - 2s - 323ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 67/100\n",
      "5/5 - 2s - 323ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 68/100\n",
      "5/5 - 2s - 350ms/step - acc: 0.8162 - loss: 0.0282 - val_loss: 0.0201\n",
      "Epoch 69/100\n",
      "5/5 - 2s - 312ms/step - acc: 0.8162 - loss: 0.0283 - val_loss: 0.0202\n",
      "Epoch 70/100\n",
      "5/5 - 2s - 308ms/step - acc: 0.8162 - loss: 0.0283 - val_loss: 0.0202\n",
      "Epoch 71/100\n",
      "5/5 - 2s - 312ms/step - acc: 0.8162 - loss: 0.0284 - val_loss: 0.0203\n",
      "Epoch 72/100\n",
      "5/5 - 2s - 312ms/step - acc: 0.8162 - loss: 0.0284 - val_loss: 0.0203\n",
      "Epoch 73/100\n",
      "5/5 - 2s - 322ms/step - acc: 0.8162 - loss: 0.0284 - val_loss: 0.0203\n",
      "Epoch 74/100\n",
      "5/5 - 1s - 300ms/step - acc: 0.8162 - loss: 0.0284 - val_loss: 0.0203\n",
      "Epoch 75/100\n",
      "5/5 - 1s - 300ms/step - acc: 0.8162 - loss: 0.0284 - val_loss: 0.0203\n",
      "Epoch 76/100\n",
      "5/5 - 2s - 300ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0203\n",
      "Epoch 77/100\n",
      "5/5 - 2s - 389ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 78/100\n",
      "5/5 - 2s - 324ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 79/100\n",
      "5/5 - 2s - 319ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 80/100\n",
      "5/5 - 2s - 301ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 81/100\n",
      "5/5 - 2s - 309ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 82/100\n",
      "5/5 - 2s - 310ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 83/100\n",
      "5/5 - 2s - 302ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0204\n",
      "Epoch 84/100\n",
      "5/5 - 2s - 304ms/step - acc: 0.8162 - loss: 0.0285 - val_loss: 0.0204\n",
      "Epoch 85/100\n",
      "5/5 - 2s - 314ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 86/100\n",
      "5/5 - 2s - 318ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0204\n",
      "Epoch 87/100\n",
      "5/5 - 2s - 325ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0204\n",
      "Epoch 88/100\n",
      "5/5 - 2s - 337ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 89/100\n",
      "5/5 - 2s - 307ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 90/100\n",
      "5/5 - 2s - 315ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 91/100\n",
      "5/5 - 2s - 303ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 92/100\n",
      "5/5 - 1s - 296ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0204\n",
      "Epoch 93/100\n",
      "5/5 - 1s - 297ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 94/100\n",
      "5/5 - 2s - 303ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 95/100\n",
      "5/5 - 2s - 302ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 96/100\n",
      "5/5 - 1s - 293ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 97/100\n",
      "5/5 - 2s - 304ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 98/100\n",
      "5/5 - 1s - 296ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 99/100\n",
      "5/5 - 1s - 297ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n",
      "Epoch 100/100\n",
      "5/5 - 2s - 302ms/step - acc: 0.8162 - loss: 0.0286 - val_loss: 0.0205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x762105cfc3b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train and evaluate\n",
    "\n",
    "\n",
    "# Define hyper-parameters\n",
    "HIDDEN_UNITS = 100\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "OUTPUT_DIM = len(class_values)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.1\n",
    "LEARNING_RATE = 3e-1\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.SGD(LEARNING_RATE, momentum=MOMENTUM)\n",
    "accuracy_fn = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", min_delta=1e-5, patience=5, restore_best_weights=True)\n",
    "\n",
    "# Build model\n",
    "gat_model = GraphAttentionNetwork(node_states, edges, HIDDEN_UNITS, NUM_HEADS, NUM_LAYERS, OUTPUT_DIM)\n",
    "\n",
    "# Compile model\n",
    "gat_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[accuracy_fn])\n",
    "\n",
    "gat_model.fit(\n",
    "    x=train_indices,\n",
    "    y=train_labels,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5e530c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Test Accuracy 79.3%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = gat_model.evaluate(x=test_indices, y=test_labels, verbose=0, return_dict=True)\n",
    "\n",
    "print(\"--\" * 38 + f\"\\nTest Accuracy {test_accuracy[\"compile_metrics\"][\"acc\"]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53858b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 149ms/step\n",
      "Example 1: Probabilistic_Methods\n",
      "\tProbability of Case_Based               =   0.040%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =   2.672%\n",
      "\tProbability of Probabilistic_Methods    =  97.261%\n",
      "\tProbability of Reinforcement_Learning   =   0.003%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.024%\n",
      "------------------------------------------------------------\n",
      "Example 2: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =  99.998%\n",
      "\tProbability of Neural_Networks          =   0.002%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 3: Theory\n",
      "\tProbability of Case_Based               =   0.542%\n",
      "\tProbability of Genetic_Algorithms       =   0.003%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   3.571%\n",
      "\tProbability of Reinforcement_Learning   =   0.098%\n",
      "\tProbability of Rule_Learning            =   1.286%\n",
      "\tProbability of Theory                   =  94.499%\n",
      "------------------------------------------------------------\n",
      "Example 4: Neural_Networks\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =  99.980%\n",
      "\tProbability of Probabilistic_Methods    =   0.020%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 5: Theory\n",
      "\tProbability of Case_Based               =   5.373%\n",
      "\tProbability of Genetic_Algorithms       =   0.471%\n",
      "\tProbability of Neural_Networks          =   2.146%\n",
      "\tProbability of Probabilistic_Methods    =  10.016%\n",
      "\tProbability of Reinforcement_Learning   =   0.384%\n",
      "\tProbability of Rule_Learning            =  65.266%\n",
      "\tProbability of Theory                   =  16.343%\n",
      "------------------------------------------------------------\n",
      "Example 6: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       = 100.000%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 7: Neural_Networks\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =  99.911%\n",
      "\tProbability of Probabilistic_Methods    =   0.088%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 8: Genetic_Algorithms\n",
      "\tProbability of Case_Based               =   0.000%\n",
      "\tProbability of Genetic_Algorithms       = 100.000%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.000%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.000%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n",
      "Example 9: Theory\n",
      "\tProbability of Case_Based               =   0.193%\n",
      "\tProbability of Genetic_Algorithms       =   0.196%\n",
      "\tProbability of Neural_Networks          =  83.411%\n",
      "\tProbability of Probabilistic_Methods    =  15.489%\n",
      "\tProbability of Reinforcement_Learning   =   0.304%\n",
      "\tProbability of Rule_Learning            =   0.075%\n",
      "\tProbability of Theory                   =   0.333%\n",
      "------------------------------------------------------------\n",
      "Example 10: Case_Based\n",
      "\tProbability of Case_Based               =  99.999%\n",
      "\tProbability of Genetic_Algorithms       =   0.000%\n",
      "\tProbability of Neural_Networks          =   0.000%\n",
      "\tProbability of Probabilistic_Methods    =   0.001%\n",
      "\tProbability of Reinforcement_Learning   =   0.000%\n",
      "\tProbability of Rule_Learning            =   0.001%\n",
      "\tProbability of Theory                   =   0.000%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Predict (probabilities)\n",
    "\n",
    "test_probs = gat_model.predict(x=test_indices)\n",
    "\n",
    "mapping = {v: k for (k, v) in class_idx.items()}\n",
    "\n",
    "for i, (probs, label) in enumerate(zip(test_probs[:10], test_labels[:10])):\n",
    "    print(f\"Example {i+1}: {mapping[label]}\")\n",
    "    for j, c in zip(probs, class_idx.keys()):\n",
    "        print(f\"\\tProbability of {c: <24} = {j*100:7.3f}%\")\n",
    "    print(\"---\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0fee0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The results look OK! The GAT model seems to correctly predict the subjects of the papers,\n",
    "based on what they cite, about 80% of the time. Further improvements could be\n",
    "made by fine-tuning the hyper-parameters of the GAT. For instance, try changing the number of layers,\n",
    "the number of hidden units, or the optimizer/learning rate; add regularization (e.g., dropout);\n",
    "or modify the preprocessing step. We could also try to implement *self-loops*\n",
    "(i.e., paper X cites paper X) and/or make the graph *undirected*.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
