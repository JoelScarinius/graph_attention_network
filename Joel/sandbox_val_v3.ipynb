{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667fe6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def find_all_scene_ids(dataset_dir):\n",
    "    scene_ids = []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\".edges\"):\n",
    "            scene_id = file.split(\".\")[0]\n",
    "            scene_ids.append(scene_id)\n",
    "    return scene_ids\n",
    "\n",
    "\n",
    "def load_all_subgraphs(dataset_dir):\n",
    "    scene_ids = find_all_scene_ids(dataset_dir)\n",
    "    scenes = []\n",
    "    for scene_id in scene_ids:\n",
    "        edges_file = os.path.join(dataset_dir, f\"{scene_id}.edges\")\n",
    "        nodes_file = os.path.join(dataset_dir, f\"{scene_id}.nodes\")\n",
    "        if not os.path.exists(edges_file) or not os.path.exists(nodes_file):\n",
    "            print(f\"Skipping scene ID {scene_id}: Missing files.\")\n",
    "            continue\n",
    "\n",
    "        edges = pd.read_csv(edges_file, sep=\",\", header=None, names=[\"target\", \"source\"])\n",
    "        nodes = pd.read_csv(\n",
    "            nodes_file,\n",
    "            sep=\",\",\n",
    "            header=None,\n",
    "            names=[\"node_id\", \"current_x\", \"current_y\", \"previous_x\", \"previous_y\", \"future_x\", \"future_y\"],\n",
    "        )\n",
    "        for col in nodes.columns:\n",
    "            nodes[col] = pd.to_numeric(nodes[col], errors=\"coerce\")\n",
    "\n",
    "        if nodes.isnull().any().any():\n",
    "            nan_nodes = nodes[nodes.isnull().any(axis=1)]\n",
    "            nan_node_ids = nan_nodes[\"node_id\"].tolist()\n",
    "            print(f\"Scene {scene_id}: Filtering {len(nan_node_ids)} nodes with NaN values.\")\n",
    "            edges = edges[~edges[\"source\"].isin(nan_node_ids) & ~edges[\"target\"].isin(nan_node_ids)]\n",
    "            nodes = nodes.dropna(subset=[\"future_x\", \"future_y\"])\n",
    "\n",
    "        if (edges[\"source\"] == -1).any() or (edges[\"target\"] == -1).any():\n",
    "            print(f\"Scene {scene_id} contains -1 edges. Removing these edges.\")\n",
    "            edges = edges[(edges[\"source\"] != -1) & (edges[\"target\"] != -1)]\n",
    "            connected_nodes = pd.unique(edges[[\"target\", \"source\"]].values.ravel())\n",
    "            nodes = nodes[nodes[\"node_id\"].isin(connected_nodes)]\n",
    "        if len(nodes) > 0:\n",
    "            scenes.append({\"scene_id\": scene_id, \"edges\": edges, \"nodes\": nodes})\n",
    "        else:\n",
    "            print(f\"NOTE! Scene {scene_id} skipped: no valid nodes after filtering.\")\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def convert_scene_to_tensors(scene, feature_cols, target_cols):\n",
    "    nodes_df = scene[\"nodes\"].reset_index(drop=True)\n",
    "    edges_df = scene[\"edges\"].reset_index(drop=True)\n",
    "    node_id_to_idx = {nid: i for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    edges_df = edges_df.copy()\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(node_id_to_idx)\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(node_id_to_idx)\n",
    "    edges_df = edges_df.dropna().astype(int)\n",
    "    features = nodes_df[feature_cols].to_numpy().astype(np.float32)\n",
    "    targets = nodes_df[target_cols].to_numpy().astype(np.float32)\n",
    "    edges = edges_df.to_numpy().astype(np.int32)\n",
    "    return features, edges, targets\n",
    "\n",
    "\n",
    "def split_scenes(scenes, train_ratio=0.7, val_ratio=0.15):\n",
    "    np.random.shuffle(scenes)\n",
    "    n_total = len(scenes)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    train_scenes = scenes[:n_train]\n",
    "    val_scenes = scenes[n_train : n_train + n_val]\n",
    "    test_scenes = scenes[n_train + n_val :]\n",
    "    return train_scenes, val_scenes, test_scenes\n",
    "\n",
    "\n",
    "def scene_generator(scene_list, feature_cols, target_cols):\n",
    "    for scene in scene_list:\n",
    "        yield convert_scene_to_tensors(scene, feature_cols, target_cols)\n",
    "\n",
    "\n",
    "def squeeze_batch(features, edges, targets):\n",
    "    return tf.squeeze(features, axis=0), tf.squeeze(edges, axis=0), tf.squeeze(targets, axis=0)\n",
    "\n",
    "\n",
    "# # 1. Fit scalers on training data\n",
    "# all_train_features = np.vstack([scene[\"nodes\"][feature_cols].values for scene in train_scenes])\n",
    "# all_train_targets = np.vstack([scene[\"nodes\"][target_cols].values for scene in train_scenes])\n",
    "# feature_scaler = StandardScaler().fit(all_train_features)\n",
    "# target_scaler = StandardScaler().fit(all_train_targets)\n",
    "\n",
    "\n",
    "# # 2. Use scalers in tensor conversion\n",
    "# def convert_scene_to_tensors(scene):\n",
    "#     nodes_df = scene[\"nodes\"].reset_index(drop=True)\n",
    "#     edges_df = scene[\"edges\"].reset_index(drop=True)\n",
    "#     node_id_to_idx = {nid: i for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "#     edges_df = edges_df.copy()\n",
    "#     edges_df[\"target\"] = edges_df[\"target\"].map(node_id_to_idx)\n",
    "#     edges_df[\"source\"] = edges_df[\"source\"].map(node_id_to_idx)\n",
    "#     edges_df = edges_df.dropna().astype(int)\n",
    "#     features = feature_scaler.transform(nodes_df[feature_cols].to_numpy().astype(np.float32))\n",
    "#     targets = target_scaler.transform(nodes_df[target_cols].to_numpy().astype(np.float32))\n",
    "#     edges = edges_df.to_numpy().astype(np.int32)\n",
    "#     return features, edges, targets\n",
    "\n",
    "\n",
    "def mean_euclidean_distance(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=-1))\n",
    "\n",
    "\n",
    "def compile_and_train(gat_model, train_dataset, val_dataset, epochs, learning_rate):\n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    metrics = [\n",
    "        keras.metrics.MeanAbsoluteError(),\n",
    "        keras.metrics.MeanSquaredError(),\n",
    "        keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "        keras.metrics.R2Score(),\n",
    "        keras.metrics.MeanMetricWrapper(mean_euclidean_distance, name=\"mean_euclidean_distance\"),\n",
    "    ]\n",
    "\n",
    "    gat_model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=1e-5, patience=15, verbose=1, restore_best_weights=True, start_from_epoch=0\n",
    "    )\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, min_delta=1e-4, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    history = gat_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    return gat_model, history\n",
    "\n",
    "\n",
    "def evaluate_and_plot(gat_model, history, test_dataset, task, run=\"1\"):\n",
    "    plot_dir = \"plots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Evaluating on test dataset...\")\n",
    "    results = gat_model.evaluate(test_dataset, verbose=2)\n",
    "    print(\"Test metrics:\", results)\n",
    "\n",
    "    print(\"\\nSample predictions for test scenes:\")\n",
    "    for features, edges, targets in test_dataset.take(1):\n",
    "        predictions = gat_model((features, edges), training=False)\n",
    "        for i in range(min(5, predictions.shape[0])):\n",
    "            print(\n",
    "                f\"Node {i}: True future_x={targets[i, 0]:.1f}, future_y={targets[i, 1]:.1f} | \"\n",
    "                f\"Predicted future_x={predictions[i, 0]:.1f}, future_y={predictions[i, 1]:.1f}\"\n",
    "            )\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(targets[:20, 0], targets[:20, 1], label=\"True\", c=\"g\")\n",
    "        plt.scatter(predictions[:20, 0], predictions[:20, 1], label=\"Pred\", c=\"r\", marker=\"x\")\n",
    "        plt.legend()\n",
    "        targets_np = targets[:20].numpy()\n",
    "        predictions_np = predictions[:20].numpy()\n",
    "\n",
    "        x_min = int(np.floor(min(targets_np[:, 0].min(), predictions_np[:, 0].min())))\n",
    "        x_max = int(np.ceil(max(targets_np[:, 0].max(), predictions_np[:, 0].max())))\n",
    "        y_min = int(np.floor(min(targets_np[:, 1].min(), predictions_np[:, 1].min())))\n",
    "        y_max = int(np.ceil(max(targets_np[:, 1].max(), predictions_np[:, 1].max())))\n",
    "\n",
    "        plt.xticks(np.arange(x_min, x_max + 1, 500), rotation=45)\n",
    "        plt.yticks(np.arange(y_min, y_max + 1, 500))\n",
    "        plt.xlabel(\"future_x\")\n",
    "        plt.ylabel(\"future_y\")\n",
    "        plt.title(\"True vs Predicted Future Positions\")\n",
    "        plt.savefig(os.path.join(plot_dir, f\"task_{task}_run_{run}_scatter.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    med = history.history[\"mean_euclidean_distance\"]\n",
    "    mse = history.history[\"mean_absolute_error\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs_range = range(len(history.history[\"val_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, med, label=\"Mean Euclidean Distance\")\n",
    "    plt.plot(epochs_range, mse, label=\"Mean Absolute Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Distance/Error\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"MED and MAE\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.savefig(os.path.join(plot_dir, f\"task_{task}_run_{run}_history.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Define Model Components\n",
    "# -------------------------\n",
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "        target_states = tf.gather(node_states_transformed, edges[:, 0])\n",
    "        source_states = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        concat_features = tf.concat([target_states, source_states], axis=-1)\n",
    "        attention_scores = tf.nn.leaky_relu(tf.matmul(concat_features, self.kernel_attention))\n",
    "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
    "        attention_scores = tf.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        num_nodes = tf.shape(node_states)[0]\n",
    "        attention_sum = tf.math.unsorted_segment_sum(attention_scores, segment_ids=edges[:, 0], num_segments=num_nodes)\n",
    "        normalized_attention = attention_scores / tf.gather(attention_sum, edges[:, 0])\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * normalized_attention[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=num_nodes,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class CosineSimilarityGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        target_states = tf.gather(node_states_transformed, edges[:, 0])\n",
    "        source_states = tf.gather(node_states_transformed, edges[:, 1])\n",
    "\n",
    "        # Normalized vectors (safe cosine similarity)\n",
    "        normalized_target = tf.math.l2_normalize(target_states, axis=-1, epsilon=1e-8)\n",
    "        normalized_source = tf.math.l2_normalize(source_states, axis=-1, epsilon=1e-8)\n",
    "\n",
    "        cosine_sim = tf.reduce_sum(normalized_target * normalized_source, axis=-1)\n",
    "\n",
    "        # Stable softmax over incoming edges\n",
    "        num_nodes = tf.shape(node_states)[0]\n",
    "        segment_max = tf.math.unsorted_segment_max(cosine_sim, edges[:, 0], num_nodes)\n",
    "        shifted_sim = cosine_sim - tf.gather(segment_max, edges[:, 0])\n",
    "        exp_sim = tf.exp(shifted_sim)\n",
    "\n",
    "        attention_sum = tf.math.unsorted_segment_sum(exp_sim, edges[:, 0], num_nodes)\n",
    "        normalized_attention = exp_sim / (tf.gather(attention_sum, edges[:, 0]) + 1e-8)\n",
    "\n",
    "        # Weighted sum of source node features\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * normalized_attention[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=num_nodes,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "        outputs = [attn([node_features, edges]) for attn in self.attention_layers]\n",
    "        if self.merge_type == \"concat\":\n",
    "            out = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            out = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(out)\n",
    "\n",
    "\n",
    "class MultiHeadCosineGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [CosineSimilarityGraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "        outputs = [attn([node_features, edges]) for attn in self.attention_layers]\n",
    "        if self.merge_type == \"concat\":\n",
    "            out = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            out = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(out)\n",
    "\n",
    "\n",
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(self, hidden_units, num_heads, num_layers, output_dim, task, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Since our dataset is unbatched, node_features is expected to have shape [n_nodes, feature_dim]\n",
    "        node_features, edges = inputs\n",
    "\n",
    "        # Directly use inputs since no batch dimension is present.\n",
    "        x = self.preprocess(node_features)\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x_new = attn_layer([x, edges])\n",
    "            x = x + x_new  # residual connection\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        node_features, edges, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([node_features, edges], training=True)\n",
    "            loss = self.compiled_loss(targets, outputs)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        node_features, edges, _ = data\n",
    "        outputs = self([node_features, edges], training=False)\n",
    "        return outputs\n",
    "\n",
    "    def test_step(self, data):\n",
    "        node_features, edges, targets = data\n",
    "        outputs = self([node_features, edges], training=False)\n",
    "        loss = self.compiled_loss(targets, outputs)\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "\n",
    "class CosineGraphAttentionNetwork(keras.Model):\n",
    "    def __init__(self, hidden_units, num_heads, num_layers, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # two‐layer preprocessing\n",
    "        self.preprocess = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(hidden_units * num_heads, activation=\"relu\"),\n",
    "                layers.Dense(hidden_units * num_heads, activation=None),\n",
    "            ]\n",
    "        )\n",
    "        # multi‐head cosine‐similarity attention stacks\n",
    "        self.attention_layers = [MultiHeadCosineGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs is now guaranteed to be (node_features, edges)\n",
    "        node_features, edges = inputs\n",
    "        x = self.preprocess(node_features)\n",
    "        for attn in self.attention_layers:\n",
    "            x_new = attn([x, edges])\n",
    "            x = x + x_new\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # unpack the (features, edges, targets) triple\n",
    "        node_features, edges, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([node_features, edges], training=True)\n",
    "            loss = self.compiled_loss(targets, outputs)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # update all metrics (including loss/MAE/RMSE/R2 etc)\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        # collect metric results\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "    def test_step(self, data):\n",
    "        node_features, edges, targets = data\n",
    "        outputs = self([node_features, edges], training=False)\n",
    "        loss = self.compiled_loss(targets, outputs)\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        node_features, edges, _ = data\n",
    "        return self([node_features, edges], training=False)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Model Training & Evaluation\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# # 3. Inverse-transform predictions for interpretability\n",
    "# for features, edges, targets in test_dataset.take(1):\n",
    "#     predictions = gat_model((features, edges), training=False)\n",
    "#     # Inverse-transform for readability\n",
    "#     predictions_orig = target_scaler.inverse_transform(predictions.numpy())\n",
    "#     targets_orig = target_scaler.inverse_transform(targets.numpy())\n",
    "#     for i in range(min(5, predictions.shape[0])):\n",
    "#         print(\n",
    "#             f\"Node {i}: True future_x={targets_orig[i, 0]:.1f}, future_y={targets_orig[i, 1]:.1f} | \"\n",
    "#             f\"Predicted future_x={predictions_orig[i, 0]:.1f}, future_y={predictions_orig[i, 1]:.1f}\"\n",
    "#         )\n",
    "\n",
    "# # --- Compute metrics in original scale ---\n",
    "\n",
    "\n",
    "# # Collect all predictions and targets\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "# for features, edges, targets in test_dataset:\n",
    "#     preds = gat_model((features, edges), training=False)\n",
    "#     all_preds.append(preds.numpy())\n",
    "#     all_targets.append(targets.numpy())\n",
    "\n",
    "# all_preds = np.vstack(all_preds)\n",
    "# all_targets = np.vstack(all_targets)\n",
    "\n",
    "# # Inverse transform\n",
    "# all_preds_orig = target_scaler.inverse_transform(all_preds)\n",
    "# all_targets_orig = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "# # Compute metrics\n",
    "# mae_orig = mean_absolute_error(all_targets_orig, all_preds_orig)\n",
    "# mse_orig = mean_squared_error(all_targets_orig, all_preds_orig)\n",
    "# euclidean_orig = np.mean(np.linalg.norm(all_targets_orig - all_preds_orig, axis=1))\n",
    "\n",
    "# print(f\"Test MAE (original scale): {mae_orig:.4f}\")\n",
    "# print(f\"Test MSE (original scale): {mse_orig:.4f}\")\n",
    "# print(f\"Test Mean Euclidean Distance (original scale): {euclidean_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1352890817715: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890817715 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814428: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890814428 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802323 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800322: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800322 contains -1 edges. Removing these edges.\n",
      "Scene 1352890875617 contains -1 edges. Removing these edges.\n",
      "Scene 1352890804562: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890804562 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841688: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890841688 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837555 contains -1 edges. Removing these edges.\n",
      "Scene 1352890825684 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801553: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801553 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832535: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832535 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829505: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829505 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803926: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803926 contains -1 edges. Removing these edges.\n",
      "Scene 1352890842354: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890842354 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813672 contains -1 edges. Removing these edges.\n",
      "Scene 1352890884154 contains -1 edges. Removing these edges.\n",
      "Scene 1352890846971: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890846971 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834297 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834338 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812682: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812682 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801292: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801292 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807487: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890807487 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815643 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808973 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834536 contains -1 edges. Removing these edges.\n",
      "Scene 1352890836794 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828841 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919291: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919291 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828882 contains -1 edges. Removing these edges.\n",
      "Scene 1352890891802 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801118 contains -1 edges. Removing these edges.\n",
      "Scene 1352890849798: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890849798 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894347: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890894347 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800459: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800459 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803486: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803486 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829713: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829713 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828668 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809063 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829002: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890918879 contains -1 edges. Removing these edges.\n",
      "Scene 1352890890812: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890890812 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837871 contains -1 edges. Removing these edges.\n",
      "Scene 135289080699: Filtering 2 nodes with NaN values.\n",
      "Scene 135289080699 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815512: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890815512 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829207: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829207 contains -1 edges. Removing these edges.\n",
      "Scene 1352890839303 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919525: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919525 contains -1 edges. Removing these edges.\n",
      "Scene 135289083242: Filtering 1 nodes with NaN values.\n",
      "Scene 135289083242 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832654: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832654 contains -1 edges. Removing these edges.\n",
      "Scene 135289082308 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832698: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832698 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818928: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818928 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809821: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809821 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803574: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890803574 contains -1 edges. Removing these edges.\n",
      "Scene 13528908000089998: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908000089998 contains -1 edges. Removing these edges.\n",
      "Scene 1352890830824 contains -1 edges. Removing these edges.\n",
      "Scene 1352890849578: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890849578 contains -1 edges. Removing these edges.\n",
      "Scene 1352890826701 contains -1 edges. Removing these edges.\n",
      "Scene 13528908087 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833057: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890833057 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809106 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834498 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919111 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832737: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832737 contains -1 edges. Removing these edges.\n",
      "Scene 135289080379: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080379 contains -1 edges. Removing these edges.\n",
      "Scene 135289091722: Filtering 1 nodes with NaN values.\n",
      "Scene 135289091722 contains -1 edges. Removing these edges.\n",
      "Scene 1352890906673: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890906673 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844758: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890844758 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844758 skipped: no valid nodes after filtering.\n",
      "Scene 1352890813256: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890813256 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801209: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801209 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818793: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818793 contains -1 edges. Removing these edges.\n",
      "Scene 13528908349910002: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908349910002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812805: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812805 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833183 contains -1 edges. Removing these edges.\n",
      "Scene 1352890886909 contains -1 edges. Removing these edges.\n",
      "Scene 135289080254: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080254 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813937 contains -1 edges. Removing these edges.\n",
      "Scene 1352890831704 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803131: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803131 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806668: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806668 contains -1 edges. Removing these edges.\n",
      "Scene 1352890805653: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890805653 contains -1 edges. Removing these edges.\n",
      "Scene 1352890805012: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890805012 contains -1 edges. Removing these edges.\n",
      "Scene 1352890878093 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806119: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806119 contains -1 edges. Removing these edges.\n",
      "Scene 1352890823766: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890823766 contains -1 edges. Removing these edges.\n",
      "Scene 135289081753 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803042: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803042 contains -1 edges. Removing these edges.\n",
      "Scene 1352890821689 contains -1 edges. Removing these edges.\n",
      "Scene 1352890835826: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890835826 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844845: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890844845 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844845 skipped: no valid nodes after filtering.\n",
      "Scene 1352890834053 contains -1 edges. Removing these edges.\n",
      "Scene 135289091765 contains -1 edges. Removing these edges.\n",
      "Scene 1352890916071 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820444 contains -1 edges. Removing these edges.\n",
      "Scene 1352890860907: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890860907 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803972: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803972 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819103: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890819103 contains -1 edges. Removing these edges.\n",
      "Scene 1352890810586 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820576 contains -1 edges. Removing these edges.\n",
      "Scene 1352890915979 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844712: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890844712 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844712 skipped: no valid nodes after filtering.\n",
      "Scene 1352890838234: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838234 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829166: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829166 contains -1 edges. Removing these edges.\n",
      "Scene 1352890845672: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890845672 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890845672 skipped: no valid nodes after filtering.\n",
      "Scene 1352890842712: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890842712 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832497: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832497 contains -1 edges. Removing these edges.\n",
      "Scene 1352890843708: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890843708 contains -1 edges. Removing these edges.\n",
      "Scene 13528908058: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908058 contains -1 edges. Removing these edges.\n",
      "Scene 1352890839439 contains -1 edges. Removing these edges.\n",
      "Scene 13528908003660002: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908003660002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824946 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824516 contains -1 edges. Removing these edges.\n",
      "Scene 135289081098 contains -1 edges. Removing these edges.\n",
      "Scene 13528908292589998: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908292589998 contains -1 edges. Removing these edges.\n",
      "Scene 1352890817934: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890817934 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800907 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815016: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890815016 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834692 contains -1 edges. Removing these edges.\n",
      "Scene 1352890867565: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890867565 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814839: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890814839 contains -1 edges. Removing these edges.\n",
      "Scene 1352890916428: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890916428 contains -1 edges. Removing these edges.\n",
      "Scene 1352890887794: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890887794 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813762 contains -1 edges. Removing these edges.\n",
      "Scene 13528908331360002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807035: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890807035 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807396: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890807396 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806949: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806949 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801417: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801417 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809687: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809687 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820225 contains -1 edges. Removing these edges.\n",
      "Scene 135289080722: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080722 contains -1 edges. Removing these edges.\n",
      "Scene 1352890891769: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890891769 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832459: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832459 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894309: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890894309 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837646 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829843: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829843 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841153: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890841153 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800768: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800768 contains -1 edges. Removing these edges.\n",
      "Scene 1352890811843: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890811843 contains -1 edges. Removing these edges.\n",
      "Scene 1352890827838 contains -1 edges. Removing these edges.\n",
      "Scene 13528908329: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908329 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803659: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890803659 contains -1 edges. Removing these edges.\n",
      "Scene 135289083891 contains -1 edges. Removing these edges.\n",
      "Scene 1352890804018: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890804018 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814167: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890814167 contains -1 edges. Removing these edges.\n",
      "Scene 1352890875117 contains -1 edges. Removing these edges.\n",
      "Scene 1352890836572 contains -1 edges. Removing these edges.\n",
      "Scene 1352890825035 contains -1 edges. Removing these edges.\n",
      "Scene 135289083596: Filtering 1 nodes with NaN values.\n",
      "Scene 135289083596 contains -1 edges. Removing these edges.\n",
      "Scene 13528908282510002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802778: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890802778 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812416: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812416 contains -1 edges. Removing these edges.\n",
      "Scene 135289083913 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832378: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832378 contains -1 edges. Removing these edges.\n",
      "Scene 1352890917968: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890917968 contains -1 edges. Removing these edges.\n",
      "Scene 13528908366160002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890823862: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890823862 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919792: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919792 contains -1 edges. Removing these edges.\n",
      "Scene 1352890872601: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890872601 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894888: Filtering 4 nodes with NaN values.\n",
      "Scene 1352890894888 contains -1 edges. Removing these edges.\n",
      "Scene 135289091426 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919969: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919969 contains -1 edges. Removing these edges.\n",
      "Scene 1352890835036: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890835036 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809912: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809912 contains -1 edges. Removing these edges.\n",
      "Scene 135289080146: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080146 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833505 contains -1 edges. Removing these edges.\n",
      "Scene 1352890895393: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890895393 contains -1 edges. Removing these edges.\n",
      "Scene 1352890838867: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838867 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833338 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819768: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890819768 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803216: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803216 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808433 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800052: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800052 contains -1 edges. Removing these edges.\n",
      "Scene 1352890811475: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890811475 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834655 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814706: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890814706 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837601 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808346 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818385: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818385 contains -1 edges. Removing these edges.\n",
      "Scene 1352890838146: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838146 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800142: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800142 contains -1 edges. Removing these edges.\n",
      "Scene 1352890851846: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890851846 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837115 contains -1 edges. Removing these edges.\n",
      "Scene 13528909148 contains -1 edges. Removing these edges.\n",
      "Scene 1352890908102: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890908102 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841295 contains -1 edges. Removing these edges.\n",
      "Scene 1352890870132: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890870132 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837204 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802191 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819948 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824206 contains -1 edges. Removing these edges.\n",
      "Scene 135289081789: Filtering 1 nodes with NaN values.\n",
      "Scene 135289081789 contains -1 edges. Removing these edges.\n",
      "Scene 1352890910955: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890910955 contains -1 edges. Removing these edges.\n",
      "Loaded 189 scenes.\n",
      "Train scenes: 132, Val scenes: 28, Test scenes: 29\n",
      "\n",
      "Running Task 3...\n",
      "\n",
      "Training...\n",
      "Epoch 1/100\n",
      "132/132 - 13s - 101ms/step - mean_absolute_error: 49221.9961 - mean_euclidean_distance: 79273.5391 - mean_squared_error: 5173923840.0000 - r2_score: -6.2291e+01 - rmse: 71929.9922 - loss: 26679515136.0000 - val_loss: 4095487232.0000 - learning_rate: 0.0100\n",
      "Epoch 2/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 10628.8086 - mean_euclidean_distance: 21691.3633 - mean_squared_error: 1119925120.0000 - r2_score: -6.2875e+00 - rmse: 33465.2812 - loss: 20270628.0000 - val_loss: 24877848.0000 - learning_rate: 0.0100\n",
      "Epoch 3/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 5964.5420 - mean_euclidean_distance: 10265.5576 - mean_squared_error: 114612120.0000 - r2_score: -1.6101e-01 - rmse: 10705.7051 - loss: 42851916.0000 - val_loss: 9751983.0000 - learning_rate: 0.0100\n",
      "Epoch 4/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 8714.6494 - mean_euclidean_distance: 15978.9092 - mean_squared_error: 404045312.0000 - r2_score: -4.6751e+00 - rmse: 20100.8789 - loss: 198589840.0000 - val_loss: 34550648.0000 - learning_rate: 0.0100\n",
      "Epoch 5/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 5561.7793 - mean_euclidean_distance: 9437.2686 - mean_squared_error: 77873816.0000 - r2_score: 0.2080 - rmse: 8824.6143 - loss: 134185376.0000 - val_loss: 16100235.0000 - learning_rate: 0.0100\n",
      "Epoch 6/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 4290.6558 - mean_euclidean_distance: 8270.5371 - mean_squared_error: 118107080.0000 - r2_score: 0.1652 - rmse: 10867.7080 - loss: 6791087.0000 - val_loss: 6429094.0000 - learning_rate: 0.0100\n",
      "Epoch 7/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 9031.9404 - mean_euclidean_distance: 14159.7598 - mean_squared_error: 153694832.0000 - r2_score: -3.5978e-01 - rmse: 12397.3721 - loss: 15424084.0000 - val_loss: 71949232.0000 - learning_rate: 0.0100\n",
      "Epoch 8/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 9079.4121 - mean_euclidean_distance: 15672.1826 - mean_squared_error: 213617776.0000 - r2_score: -1.5464e+00 - rmse: 14615.6689 - loss: 379463936.0000 - val_loss: 51800008.0000 - learning_rate: 0.0100\n",
      "Epoch 9/100\n",
      "132/132 - 1s - 9ms/step - mean_absolute_error: 13722.1221 - mean_euclidean_distance: 21458.8809 - mean_squared_error: 299436768.0000 - r2_score: -2.2223e+00 - rmse: 17304.2422 - loss: 103064376.0000 - val_loss: 95821616.0000 - learning_rate: 0.0100\n",
      "Epoch 10/100\n",
      "132/132 - 1s - 10ms/step - mean_absolute_error: 2303.6750 - mean_euclidean_distance: 3775.0601 - mean_squared_error: 17754066.0000 - r2_score: 0.7088 - rmse: 4213.5576 - loss: 8783331.0000 - val_loss: 1617918.3750 - learning_rate: 0.0100\n",
      "Epoch 11/100\n",
      "132/132 - 1s - 11ms/step - mean_absolute_error: 6843.3960 - mean_euclidean_distance: 10722.9287 - mean_squared_error: 95466816.0000 - r2_score: 0.1079 - rmse: 9770.7119 - loss: 4292197.5000 - val_loss: 40209856.0000 - learning_rate: 0.0100\n",
      "Epoch 12/100\n",
      "132/132 - 1s - 11ms/step - mean_absolute_error: 3046.6855 - mean_euclidean_distance: 5357.6177 - mean_squared_error: 24533138.0000 - r2_score: 0.7393 - rmse: 4953.0938 - loss: 3536702.7500 - val_loss: 3431326.7500 - learning_rate: 0.0100\n",
      "Epoch 13/100\n",
      "132/132 - 2s - 11ms/step - mean_absolute_error: 6853.7798 - mean_euclidean_distance: 10526.6709 - mean_squared_error: 70586048.0000 - r2_score: 0.3017 - rmse: 8401.5508 - loss: 228909632.0000 - val_loss: 27275188.0000 - learning_rate: 0.0100\n",
      "Epoch 14/100\n",
      "132/132 - 1s - 11ms/step - mean_absolute_error: 3441.0552 - mean_euclidean_distance: 5463.6226 - mean_squared_error: 23977148.0000 - r2_score: 0.5906 - rmse: 4896.6465 - loss: 7401473.5000 - val_loss: 6778344.0000 - learning_rate: 0.0100\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "132/132 - 1s - 11ms/step - mean_absolute_error: 2765.8667 - mean_euclidean_distance: 4621.9033 - mean_squared_error: 22954580.0000 - r2_score: 0.7382 - rmse: 4791.0938 - loss: 20177416.0000 - val_loss: 3490010.5000 - learning_rate: 0.0100\n",
      "Epoch 16/100\n",
      "132/132 - 1s - 11ms/step - mean_absolute_error: 1301.1353 - mean_euclidean_distance: 2328.0200 - mean_squared_error: 8857127.0000 - r2_score: 0.8409 - rmse: 2976.0925 - loss: 1272799.1250 - val_loss: 874190.5000 - learning_rate: 1.0000e-03\n",
      "Epoch 17/100\n",
      "132/132 - 2s - 12ms/step - mean_absolute_error: 1283.1497 - mean_euclidean_distance: 2331.8809 - mean_squared_error: 8493029.0000 - r2_score: 0.8468 - rmse: 2914.2803 - loss: 33863892.0000 - val_loss: 881489.8125 - learning_rate: 1.0000e-03\n",
      "Epoch 18/100\n",
      "132/132 - 2s - 12ms/step - mean_absolute_error: 1304.7142 - mean_euclidean_distance: 2346.6855 - mean_squared_error: 8638145.0000 - r2_score: 0.8431 - rmse: 2939.0720 - loss: 621603.1250 - val_loss: 666276.1875 - learning_rate: 1.0000e-03\n",
      "Epoch 19/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1282.3115 - mean_euclidean_distance: 2329.0156 - mean_squared_error: 8457203.0000 - r2_score: 0.8468 - rmse: 2908.1270 - loss: 16298277.0000 - val_loss: 841474.6250 - learning_rate: 1.0000e-03\n",
      "Epoch 20/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1291.3141 - mean_euclidean_distance: 2324.4319 - mean_squared_error: 8595539.0000 - r2_score: 0.8456 - rmse: 2931.8149 - loss: 3782975.0000 - val_loss: 985921.0000 - learning_rate: 1.0000e-03\n",
      "Epoch 21/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1282.5552 - mean_euclidean_distance: 2319.8345 - mean_squared_error: 8513392.0000 - r2_score: 0.8445 - rmse: 2917.7717 - loss: 1985933.6250 - val_loss: 739894.3750 - learning_rate: 1.0000e-03\n",
      "Epoch 22/100\n",
      "132/132 - 2s - 14ms/step - mean_absolute_error: 1266.3595 - mean_euclidean_distance: 2255.8794 - mean_squared_error: 8458218.0000 - r2_score: 0.8493 - rmse: 2908.3015 - loss: 2946771.0000 - val_loss: 937096.8125 - learning_rate: 1.0000e-03\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1235.7283 - mean_euclidean_distance: 2201.6191 - mean_squared_error: 8266523.5000 - r2_score: 0.8531 - rmse: 2875.1562 - loss: 2023280.3750 - val_loss: 924752.8125 - learning_rate: 1.0000e-03\n",
      "Epoch 24/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1213.8856 - mean_euclidean_distance: 2187.7500 - mean_squared_error: 8231502.0000 - r2_score: 0.8533 - rmse: 2869.0593 - loss: 13113146.0000 - val_loss: 779309.1875 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1205.0044 - mean_euclidean_distance: 2163.9412 - mean_squared_error: 8217036.0000 - r2_score: 0.8532 - rmse: 2866.5374 - loss: 17409138.0000 - val_loss: 739882.3750 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1202.3260 - mean_euclidean_distance: 2155.8701 - mean_squared_error: 8217659.0000 - r2_score: 0.8533 - rmse: 2866.6460 - loss: 2917861.7500 - val_loss: 744049.1250 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1202.6455 - mean_euclidean_distance: 2162.7146 - mean_squared_error: 8240077.5000 - r2_score: 0.8526 - rmse: 2870.5535 - loss: 1689021.8750 - val_loss: 786359.9375 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1199.4424 - mean_euclidean_distance: 2153.9653 - mean_squared_error: 8231583.5000 - r2_score: 0.8531 - rmse: 2869.0737 - loss: 2363071.7500 - val_loss: 771689.9375 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1199.1255 - mean_euclidean_distance: 2153.5293 - mean_squared_error: 8225746.5000 - r2_score: 0.8531 - rmse: 2868.0562 - loss: 1797530.7500 - val_loss: 762807.6250 - learning_rate: 1.0000e-05\n",
      "Epoch 30/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1198.9751 - mean_euclidean_distance: 2151.2363 - mean_squared_error: 8224019.0000 - r2_score: 0.8532 - rmse: 2867.7551 - loss: 1695176.3750 - val_loss: 763333.3125 - learning_rate: 1.0000e-05\n",
      "Epoch 31/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1198.7488 - mean_euclidean_distance: 2152.8938 - mean_squared_error: 8222172.0000 - r2_score: 0.8532 - rmse: 2867.4331 - loss: 9846577.0000 - val_loss: 757590.0000 - learning_rate: 1.0000e-05\n",
      "Epoch 32/100\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1198.6439 - mean_euclidean_distance: 2153.0408 - mean_squared_error: 8223879.0000 - r2_score: 0.8531 - rmse: 2867.7307 - loss: 485755.8750 - val_loss: 759259.9375 - learning_rate: 1.0000e-05\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "132/132 - 2s - 13ms/step - mean_absolute_error: 1198.2332 - mean_euclidean_distance: 2151.6133 - mean_squared_error: 8219906.5000 - r2_score: 0.8533 - rmse: 2867.0378 - loss: 2350551.0000 - val_loss: 760125.9375 - learning_rate: 1.0000e-05\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "Evaluating on test dataset...\n",
      "29/29 - 0s - 6ms/step - mean_absolute_error: 1646.3971 - mean_euclidean_distance: 2700.7812 - mean_squared_error: 30399574.0000 - r2_score: 0.5761 - rmse: 5513.5811 - loss: 1092846.0000\n",
      "Test metrics: [<tf.Tensor: shape=(), dtype=float32, numpy=1092846.0>, {'mean_absolute_error': <tf.Tensor: shape=(), dtype=float32, numpy=1646.3970947265625>, 'mean_squared_error': <tf.Tensor: shape=(), dtype=float32, numpy=30399574.0>, 'rmse': <tf.Tensor: shape=(), dtype=float32, numpy=5513.5810546875>, 'r2_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.5760504007339478>, 'mean_euclidean_distance': <tf.Tensor: shape=(), dtype=float32, numpy=2700.78125>}]\n",
      "\n",
      "Sample predictions for test scenes:\n",
      "Node 0: True future_x=40149.0, future_y=-16841.0 | Predicted future_x=43148.5, future_y=-18664.5\n",
      "Node 1: True future_x=36150.0, future_y=-21965.0 | Predicted future_x=37579.8, future_y=-23145.1\n",
      "Node 2: True future_x=42131.0, future_y=-21564.0 | Predicted future_x=41840.9, future_y=-21855.5\n",
      "Node 3: True future_x=42021.0, future_y=-23271.0 | Predicted future_x=41971.0, future_y=-23032.8\n",
      "Node 4: True future_x=42498.0, future_y=-21046.0 | Predicted future_x=43019.1, future_y=-21141.6\n",
      "Running time: 0 hours, 1 minutes, 3 seconds\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "\n",
    "tasks = [1, 2, 3]\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    feature_cols = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "    target_cols = [\"future_x\", \"future_y\"]\n",
    "\n",
    "    scenes = load_all_subgraphs(dataset_dir)\n",
    "    print(f\"Loaded {len(scenes)} scenes.\")\n",
    "    train_scenes, val_scenes, test_scenes = split_scenes(scenes, train_ratio=0.7, val_ratio=0.15)\n",
    "    print(f\"Train scenes: {len(train_scenes)}, Val scenes: {len(val_scenes)}, Test scenes: {len(test_scenes)}\")\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(train_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(val_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(test_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(100).batch(1).map(squeeze_batch)\n",
    "    val_dataset = val_dataset.batch(1).map(squeeze_batch)\n",
    "    test_dataset = test_dataset.batch(1).map(squeeze_batch)\n",
    "\n",
    "    HIDDEN_UNITS = 100\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 3\n",
    "    OUTPUT_DIM = 2\n",
    "    LEARNING_RATE = 1e-2\n",
    "    NUM_EPOCHS = 100\n",
    "\n",
    "    gat_model = None\n",
    "    history = None\n",
    "\n",
    "    if task == 1:\n",
    "        print(\"\\nRunning Task 1...\\n\")\n",
    "\n",
    "        gat_model = GraphAttentionNetwork(\n",
    "            hidden_units=HIDDEN_UNITS, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, output_dim=OUTPUT_DIM, task=task\n",
    "        )\n",
    "\n",
    "    elif task == 2:\n",
    "        print(\"\\nRunning Task 2...\\n\")\n",
    "        num_heads = [4, 8, 16]\n",
    "\n",
    "        for i, heads in enumerate(num_heads):\n",
    "\n",
    "            gat_model = GraphAttentionNetwork(\n",
    "                hidden_units=HIDDEN_UNITS, num_heads=heads, num_layers=NUM_LAYERS, output_dim=OUTPUT_DIM, task=task\n",
    "            )\n",
    "\n",
    "            gat_model, history = compile_and_train(\n",
    "                gat_model=gat_model,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "            )\n",
    "\n",
    "            evaluate_and_plot(gat_model=gat_model, history=history, test_dataset=test_dataset, task=task)\n",
    "\n",
    "    elif task == 3:\n",
    "        print(\"\\nRunning Task 3...\\n\")\n",
    "\n",
    "        gat_model = CosineGraphAttentionNetwork(\n",
    "            hidden_units=HIDDEN_UNITS,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task\")\n",
    "    \n",
    "    if task != 2:\n",
    "        gat_model, history = compile_and_train(\n",
    "            gat_model=gat_model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "    \n",
    "        evaluate_and_plot(gat_model=gat_model, history=history, test_dataset=test_dataset, task=task)\n",
    "\n",
    "end_time = time.time()\n",
    "running_time = end_time - start_time\n",
    "hours = int(running_time // 3600)\n",
    "minutes = int((running_time % 3600) // 60)\n",
    "seconds = int(running_time % 60)\n",
    "print(f\"Running time: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
