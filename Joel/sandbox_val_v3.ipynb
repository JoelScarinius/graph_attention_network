{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667fe6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def find_all_scene_ids(dataset_dir):\n",
    "    scene_ids = []\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        if file.endswith(\".edges\"):\n",
    "            scene_id = file.split(\".\")[0]\n",
    "            scene_ids.append(scene_id)\n",
    "    return scene_ids\n",
    "\n",
    "\n",
    "def load_all_subgraphs(dataset_dir):\n",
    "    scene_ids = find_all_scene_ids(dataset_dir)\n",
    "    scenes = []\n",
    "    for scene_id in scene_ids:\n",
    "        edges_file = os.path.join(dataset_dir, f\"{scene_id}.edges\")\n",
    "        nodes_file = os.path.join(dataset_dir, f\"{scene_id}.nodes\")\n",
    "        if not os.path.exists(edges_file) or not os.path.exists(nodes_file):\n",
    "            print(f\"Skipping scene ID {scene_id}: Missing files.\")\n",
    "            continue\n",
    "\n",
    "        edges = pd.read_csv(edges_file, sep=\",\", header=None, names=[\"target\", \"source\"])\n",
    "        nodes = pd.read_csv(\n",
    "            nodes_file,\n",
    "            sep=\",\",\n",
    "            header=None,\n",
    "            names=[\"node_id\", \"current_x\", \"current_y\", \"previous_x\", \"previous_y\", \"future_x\", \"future_y\"],\n",
    "        )\n",
    "        for col in nodes.columns:\n",
    "            nodes[col] = pd.to_numeric(nodes[col], errors=\"coerce\")\n",
    "\n",
    "        if nodes.isnull().any().any():\n",
    "            nan_nodes = nodes[nodes.isnull().any(axis=1)]\n",
    "            nan_node_ids = nan_nodes[\"node_id\"].tolist()\n",
    "            print(f\"Scene {scene_id}: Filtering {len(nan_node_ids)} nodes with NaN values.\")\n",
    "            edges = edges[~edges[\"source\"].isin(nan_node_ids) & ~edges[\"target\"].isin(nan_node_ids)]\n",
    "            nodes = nodes.dropna(subset=[\"future_x\", \"future_y\"])\n",
    "\n",
    "        if (edges[\"source\"] == -1).any() or (edges[\"target\"] == -1).any():\n",
    "            print(f\"Scene {scene_id} contains -1 edges. Removing these edges.\")\n",
    "            edges = edges[(edges[\"source\"] != -1) & (edges[\"target\"] != -1)]\n",
    "            connected_nodes = pd.unique(edges[[\"target\", \"source\"]].values.ravel())\n",
    "            nodes = nodes[nodes[\"node_id\"].isin(connected_nodes)]\n",
    "        if len(nodes) > 0:\n",
    "            scenes.append({\"scene_id\": scene_id, \"edges\": edges, \"nodes\": nodes})\n",
    "        else:\n",
    "            print(f\"NOTE! Scene {scene_id} skipped: no valid nodes after filtering.\")\n",
    "    return scenes\n",
    "\n",
    "\n",
    "def convert_scene_to_tensors(scene, feature_cols, target_cols):\n",
    "    nodes_df = scene[\"nodes\"].reset_index(drop=True)\n",
    "    edges_df = scene[\"edges\"].reset_index(drop=True)\n",
    "    node_id_to_idx = {nid: i for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "    edges_df = edges_df.copy()\n",
    "    edges_df[\"target\"] = edges_df[\"target\"].map(node_id_to_idx)\n",
    "    edges_df[\"source\"] = edges_df[\"source\"].map(node_id_to_idx)\n",
    "    edges_df = edges_df.dropna().astype(int)\n",
    "    features = nodes_df[feature_cols].to_numpy().astype(np.float32)\n",
    "    targets = nodes_df[target_cols].to_numpy().astype(np.float32)\n",
    "    edges = edges_df.to_numpy().astype(np.int32)\n",
    "    return features, edges, targets\n",
    "\n",
    "\n",
    "def split_scenes(scenes, train_ratio=0.7, val_ratio=0.15):\n",
    "    np.random.shuffle(scenes)\n",
    "    n_total = len(scenes)\n",
    "    n_train = int(n_total * train_ratio)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    train_scenes = scenes[:n_train]\n",
    "    val_scenes = scenes[n_train : n_train + n_val]\n",
    "    test_scenes = scenes[n_train + n_val :]\n",
    "    return train_scenes, val_scenes, test_scenes\n",
    "\n",
    "\n",
    "def scene_generator(scene_list, feature_cols, target_cols):\n",
    "    for scene in scene_list:\n",
    "        yield convert_scene_to_tensors(scene, feature_cols, target_cols)\n",
    "\n",
    "\n",
    "def squeeze_batch(features, edges, targets):\n",
    "    return tf.squeeze(features, axis=0), tf.squeeze(edges, axis=0), tf.squeeze(targets, axis=0)\n",
    "\n",
    "\n",
    "# # 1. Fit scalers on training data\n",
    "# all_train_features = np.vstack([scene[\"nodes\"][feature_cols].values for scene in train_scenes])\n",
    "# all_train_targets = np.vstack([scene[\"nodes\"][target_cols].values for scene in train_scenes])\n",
    "# feature_scaler = StandardScaler().fit(all_train_features)\n",
    "# target_scaler = StandardScaler().fit(all_train_targets)\n",
    "\n",
    "\n",
    "# # 2. Use scalers in tensor conversion\n",
    "# def convert_scene_to_tensors(scene):\n",
    "#     nodes_df = scene[\"nodes\"].reset_index(drop=True)\n",
    "#     edges_df = scene[\"edges\"].reset_index(drop=True)\n",
    "#     node_id_to_idx = {nid: i for i, nid in enumerate(nodes_df[\"node_id\"])}\n",
    "#     edges_df = edges_df.copy()\n",
    "#     edges_df[\"target\"] = edges_df[\"target\"].map(node_id_to_idx)\n",
    "#     edges_df[\"source\"] = edges_df[\"source\"].map(node_id_to_idx)\n",
    "#     edges_df = edges_df.dropna().astype(int)\n",
    "#     features = feature_scaler.transform(nodes_df[feature_cols].to_numpy().astype(np.float32))\n",
    "#     targets = target_scaler.transform(nodes_df[target_cols].to_numpy().astype(np.float32))\n",
    "#     edges = edges_df.to_numpy().astype(np.int32)\n",
    "#     return features, edges, targets\n",
    "\n",
    "\n",
    "def mean_euclidean_distance(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_true - y_pred, axis=-1))\n",
    "\n",
    "\n",
    "def compile_and_train(gat_model, train_dataset, val_dataset, epochs, learning_rate):\n",
    "    loss_fn = keras.losses.MeanSquaredError()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    metrics = [\n",
    "        keras.metrics.MeanAbsoluteError(),\n",
    "        keras.metrics.MeanSquaredError(),\n",
    "        keras.metrics.RootMeanSquaredError(name=\"rmse\"),\n",
    "        keras.metrics.R2Score(),\n",
    "        keras.metrics.MeanMetricWrapper(mean_euclidean_distance, name=\"mean_euclidean_distance\"),\n",
    "    ]\n",
    "\n",
    "    gat_model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", min_delta=1e-5, patience=15, verbose=1, restore_best_weights=True, start_from_epoch=0\n",
    "    )\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.1, patience=5, verbose=1, min_delta=1e-4, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    history = gat_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    return gat_model, history\n",
    "\n",
    "\n",
    "def evaluate_and_plot(gat_model, history, test_dataset, task, run=\"1\"):\n",
    "    plot_dir = \"plots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Evaluating on test dataset...\")\n",
    "    results = gat_model.evaluate(test_dataset, verbose=2)\n",
    "    print(\"Test metrics:\", results)\n",
    "\n",
    "    print(\"\\nSample predictions for test scenes:\")\n",
    "    for features, edges, targets in test_dataset.take(1):\n",
    "        predictions = gat_model((features, edges), training=False)\n",
    "        for i in range(min(5, predictions.shape[0])):\n",
    "            print(\n",
    "                f\"Node {i}: True future_x={targets[i, 0]:.1f}, future_y={targets[i, 1]:.1f} | \"\n",
    "                f\"Predicted future_x={predictions[i, 0]:.1f}, future_y={predictions[i, 1]:.1f}\"\n",
    "            )\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(targets[:20, 0], targets[:20, 1], label=\"True\", c=\"g\")\n",
    "        plt.scatter(predictions[:20, 0], predictions[:20, 1], label=\"Pred\", c=\"r\", marker=\"x\")\n",
    "        plt.legend()\n",
    "        targets_np = targets[:20].numpy()\n",
    "        predictions_np = predictions[:20].numpy()\n",
    "\n",
    "        x_min = int(np.floor(min(targets_np[:, 0].min(), predictions_np[:, 0].min())))\n",
    "        x_max = int(np.ceil(max(targets_np[:, 0].max(), predictions_np[:, 0].max())))\n",
    "        y_min = int(np.floor(min(targets_np[:, 1].min(), predictions_np[:, 1].min())))\n",
    "        y_max = int(np.ceil(max(targets_np[:, 1].max(), predictions_np[:, 1].max())))\n",
    "\n",
    "        plt.xticks(np.arange(x_min, x_max + 1, 500), rotation=45)\n",
    "        plt.yticks(np.arange(y_min, y_max + 1, 500))\n",
    "        plt.xlabel(\"future_x\")\n",
    "        plt.ylabel(\"future_y\")\n",
    "        plt.title(\"True vs Predicted Future Positions\")\n",
    "        plt.savefig(os.path.join(plot_dir, f\"task_{task}_run_{run}_scatter.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    med = history.history[\"mean_euclidean_distance\"]\n",
    "    mse = history.history[\"mean_absolute_error\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs_range = range(len(history.history[\"val_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, med, label=\"Mean Euclidean Distance\")\n",
    "    plt.plot(epochs_range, mse, label=\"Mean Absolute Error\")\n",
    "    plt.yticks(range(0, 6500, 500), minor=False)\n",
    "    plt.yticks(range(0, 6500, 100), minor=True)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Distance/Error\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"MED and MAE\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "    plt.savefig(os.path.join(plot_dir, f\"task_{task}_run_{run}_history.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Define Model Components\n",
    "# -------------------------\n",
    "class GraphAttention(layers.Layer):\n",
    "    def __init__(self, units, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.kernel_attention = self.add_weight(\n",
    "            shape=(self.units * 2, 1),\n",
    "            trainable=True,\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            name=\"kernel_attention\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "        target_states = tf.gather(node_states_transformed, edges[:, 0])\n",
    "        source_states = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        concat_features = tf.concat([target_states, source_states], axis=-1)\n",
    "        attention_scores = tf.nn.leaky_relu(tf.matmul(concat_features, self.kernel_attention))\n",
    "        attention_scores = tf.squeeze(attention_scores, axis=-1)\n",
    "        attention_scores = tf.exp(tf.clip_by_value(attention_scores, -2, 2))\n",
    "        num_nodes = tf.shape(node_states)[0]\n",
    "        attention_sum = tf.math.unsorted_segment_sum(attention_scores, segment_ids=edges[:, 0], num_segments=num_nodes)\n",
    "        normalized_attention = attention_scores / tf.gather(attention_sum, edges[:, 0])\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * normalized_attention[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=num_nodes,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class CosineSimilarityGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self.units),\n",
    "            trainable=True,\n",
    "            initializer=\"glorot_uniform\",\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_states, edges = inputs\n",
    "        node_states_transformed = tf.matmul(node_states, self.kernel)\n",
    "\n",
    "        target_states = tf.gather(node_states_transformed, edges[:, 0])\n",
    "        source_states = tf.gather(node_states_transformed, edges[:, 1])\n",
    "\n",
    "        # Normalized vectors (safe cosine similarity)\n",
    "        normalized_target = tf.math.l2_normalize(target_states, axis=-1, epsilon=1e-8)\n",
    "        normalized_source = tf.math.l2_normalize(source_states, axis=-1, epsilon=1e-8)\n",
    "\n",
    "        cosine_sim = tf.reduce_sum(normalized_target * normalized_source, axis=-1)\n",
    "\n",
    "        # Stable softmax over incoming edges\n",
    "        num_nodes = tf.shape(node_states)[0]\n",
    "        segment_max = tf.math.unsorted_segment_max(cosine_sim, edges[:, 0], num_nodes)\n",
    "        shifted_sim = cosine_sim - tf.gather(segment_max, edges[:, 0])\n",
    "        exp_sim = tf.exp(shifted_sim)\n",
    "\n",
    "        attention_sum = tf.math.unsorted_segment_sum(exp_sim, edges[:, 0], num_nodes)\n",
    "        normalized_attention = exp_sim / (tf.gather(attention_sum, edges[:, 0]) + 1e-8)\n",
    "\n",
    "        # Weighted sum of source node features\n",
    "        node_states_neighbors = tf.gather(node_states_transformed, edges[:, 1])\n",
    "        out = tf.math.unsorted_segment_sum(\n",
    "            data=node_states_neighbors * normalized_attention[:, tf.newaxis],\n",
    "            segment_ids=edges[:, 0],\n",
    "            num_segments=num_nodes,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [GraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "        outputs = [attn([node_features, edges]) for attn in self.attention_layers]\n",
    "        if self.merge_type == \"concat\":\n",
    "            out = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            out = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(out)\n",
    "\n",
    "\n",
    "class MultiHeadCosineGraphAttention(layers.Layer):\n",
    "    def __init__(self, units, num_heads=8, merge_type=\"concat\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.merge_type = merge_type\n",
    "        self.attention_layers = [CosineSimilarityGraphAttention(units) for _ in range(num_heads)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        node_features, edges = inputs\n",
    "        outputs = [attn([node_features, edges]) for attn in self.attention_layers]\n",
    "        if self.merge_type == \"concat\":\n",
    "            out = tf.concat(outputs, axis=-1)\n",
    "        else:\n",
    "            out = tf.reduce_mean(tf.stack(outputs, axis=-1), axis=-1)\n",
    "        return tf.nn.relu(out)\n",
    "\n",
    "\n",
    "class GraphAttentionNetwork(keras.Model):\n",
    "    def __init__(self, hidden_units, num_heads, num_layers, output_dim, task, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.preprocess = layers.Dense(hidden_units * num_heads, activation=\"relu\")\n",
    "        self.attention_layers = [MultiHeadGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Since our dataset is unbatched, node_features is expected to have shape [n_nodes, feature_dim]\n",
    "        node_features, edges = inputs\n",
    "\n",
    "        # Directly use inputs since no batch dimension is present.\n",
    "        x = self.preprocess(node_features)\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x_new = attn_layer([x, edges])\n",
    "            x = x + x_new  # residual connection\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        node_features, edges, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([node_features, edges], training=True)\n",
    "            loss = self.compiled_loss(targets, outputs)\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        node_features, edges, _ = data\n",
    "        outputs = self([node_features, edges], training=False)\n",
    "        return outputs\n",
    "\n",
    "    def test_step(self, data):\n",
    "        node_features, edges, targets = data\n",
    "        outputs = self([node_features, edges], training=False)\n",
    "        loss = self.compiled_loss(targets, outputs)\n",
    "        self.compiled_metrics.update_state(targets, outputs)\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs[\"loss\"] = loss\n",
    "        return logs\n",
    "\n",
    "\n",
    "class CosineGraphAttentionNetwork(keras.Model):\n",
    "    def __init__(self, hidden_units, num_heads, num_layers, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.preprocess = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(hidden_units * num_heads, activation=\"relu\"),\n",
    "                layers.Dense(hidden_units * num_heads, activation=None),\n",
    "            ]\n",
    "        )\n",
    "        self.attention_layers = [MultiHeadCosineGraphAttention(hidden_units, num_heads) for _ in range(num_layers)]\n",
    "        self.output_layer = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # node_features, edges = inputs\n",
    "        node_features = inputs[0]\n",
    "        edges = inputs[1]\n",
    "        x = self.preprocess(node_features)\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x_new = attn_layer([x, edges])\n",
    "            x = x + x_new\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "# -------------------------\n",
    "# Model Training & Evaluation\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# # 3. Inverse-transform predictions for interpretability\n",
    "# for features, edges, targets in test_dataset.take(1):\n",
    "#     predictions = gat_model((features, edges), training=False)\n",
    "#     # Inverse-transform for readability\n",
    "#     predictions_orig = target_scaler.inverse_transform(predictions.numpy())\n",
    "#     targets_orig = target_scaler.inverse_transform(targets.numpy())\n",
    "#     for i in range(min(5, predictions.shape[0])):\n",
    "#         print(\n",
    "#             f\"Node {i}: True future_x={targets_orig[i, 0]:.1f}, future_y={targets_orig[i, 1]:.1f} | \"\n",
    "#             f\"Predicted future_x={predictions_orig[i, 0]:.1f}, future_y={predictions_orig[i, 1]:.1f}\"\n",
    "#         )\n",
    "\n",
    "# # --- Compute metrics in original scale ---\n",
    "\n",
    "\n",
    "# # Collect all predictions and targets\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "# for features, edges, targets in test_dataset:\n",
    "#     preds = gat_model((features, edges), training=False)\n",
    "#     all_preds.append(preds.numpy())\n",
    "#     all_targets.append(targets.numpy())\n",
    "\n",
    "# all_preds = np.vstack(all_preds)\n",
    "# all_targets = np.vstack(all_targets)\n",
    "\n",
    "# # Inverse transform\n",
    "# all_preds_orig = target_scaler.inverse_transform(all_preds)\n",
    "# all_targets_orig = target_scaler.inverse_transform(all_targets)\n",
    "\n",
    "# # Compute metrics\n",
    "# mae_orig = mean_absolute_error(all_targets_orig, all_preds_orig)\n",
    "# mse_orig = mean_squared_error(all_targets_orig, all_preds_orig)\n",
    "# euclidean_orig = np.mean(np.linalg.norm(all_targets_orig - all_preds_orig, axis=1))\n",
    "\n",
    "# print(f\"Test MAE (original scale): {mae_orig:.4f}\")\n",
    "# print(f\"Test MSE (original scale): {mse_orig:.4f}\")\n",
    "# print(f\"Test Mean Euclidean Distance (original scale): {euclidean_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1803a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1352890817715: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890817715 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814428: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890814428 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802323 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800322: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800322 contains -1 edges. Removing these edges.\n",
      "Scene 1352890875617 contains -1 edges. Removing these edges.\n",
      "Scene 1352890804562: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890804562 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841688: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890841688 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837555 contains -1 edges. Removing these edges.\n",
      "Scene 1352890825684 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801553: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801553 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832535: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832535 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829505: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829505 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803926: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803926 contains -1 edges. Removing these edges.\n",
      "Scene 1352890842354: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890842354 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813672 contains -1 edges. Removing these edges.\n",
      "Scene 1352890884154 contains -1 edges. Removing these edges.\n",
      "Scene 1352890846971: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890846971 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834297 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834338 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812682: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812682 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801292: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801292 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807487: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890807487 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815643 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808973 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834536 contains -1 edges. Removing these edges.\n",
      "Scene 1352890836794 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828841 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919291: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919291 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828882 contains -1 edges. Removing these edges.\n",
      "Scene 1352890891802 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801118 contains -1 edges. Removing these edges.\n",
      "Scene 1352890849798: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890849798 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894347: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890894347 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800459: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800459 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803486: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803486 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829713: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829713 contains -1 edges. Removing these edges.\n",
      "Scene 1352890828668 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809063 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829002: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890918879 contains -1 edges. Removing these edges.\n",
      "Scene 1352890890812: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890890812 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837871 contains -1 edges. Removing these edges.\n",
      "Scene 135289080699: Filtering 2 nodes with NaN values.\n",
      "Scene 135289080699 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815512: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890815512 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829207: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829207 contains -1 edges. Removing these edges.\n",
      "Scene 1352890839303 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919525: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919525 contains -1 edges. Removing these edges.\n",
      "Scene 135289083242: Filtering 1 nodes with NaN values.\n",
      "Scene 135289083242 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832654: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832654 contains -1 edges. Removing these edges.\n",
      "Scene 135289082308 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832698: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832698 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818928: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818928 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809821: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809821 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803574: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890803574 contains -1 edges. Removing these edges.\n",
      "Scene 13528908000089998: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908000089998 contains -1 edges. Removing these edges.\n",
      "Scene 1352890830824 contains -1 edges. Removing these edges.\n",
      "Scene 1352890849578: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890849578 contains -1 edges. Removing these edges.\n",
      "Scene 1352890826701 contains -1 edges. Removing these edges.\n",
      "Scene 13528908087 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833057: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890833057 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809106 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834498 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919111 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832737: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832737 contains -1 edges. Removing these edges.\n",
      "Scene 135289080379: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080379 contains -1 edges. Removing these edges.\n",
      "Scene 135289091722: Filtering 1 nodes with NaN values.\n",
      "Scene 135289091722 contains -1 edges. Removing these edges.\n",
      "Scene 1352890906673: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890906673 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844758: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890844758 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844758 skipped: no valid nodes after filtering.\n",
      "Scene 1352890813256: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890813256 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801209: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801209 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818793: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818793 contains -1 edges. Removing these edges.\n",
      "Scene 13528908349910002: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908349910002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812805: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812805 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833183 contains -1 edges. Removing these edges.\n",
      "Scene 1352890886909 contains -1 edges. Removing these edges.\n",
      "Scene 135289080254: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080254 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813937 contains -1 edges. Removing these edges.\n",
      "Scene 1352890831704 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803131: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803131 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806668: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806668 contains -1 edges. Removing these edges.\n",
      "Scene 1352890805653: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890805653 contains -1 edges. Removing these edges.\n",
      "Scene 1352890805012: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890805012 contains -1 edges. Removing these edges.\n",
      "Scene 1352890878093 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806119: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806119 contains -1 edges. Removing these edges.\n",
      "Scene 1352890823766: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890823766 contains -1 edges. Removing these edges.\n",
      "Scene 135289081753 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803042: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803042 contains -1 edges. Removing these edges.\n",
      "Scene 1352890821689 contains -1 edges. Removing these edges.\n",
      "Scene 1352890835826: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890835826 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844845: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890844845 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844845 skipped: no valid nodes after filtering.\n",
      "Scene 1352890834053 contains -1 edges. Removing these edges.\n",
      "Scene 135289091765 contains -1 edges. Removing these edges.\n",
      "Scene 1352890916071 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820444 contains -1 edges. Removing these edges.\n",
      "Scene 1352890860907: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890860907 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803972: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803972 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819103: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890819103 contains -1 edges. Removing these edges.\n",
      "Scene 1352890810586 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820576 contains -1 edges. Removing these edges.\n",
      "Scene 1352890915979 contains -1 edges. Removing these edges.\n",
      "Scene 1352890844712: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890844712 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890844712 skipped: no valid nodes after filtering.\n",
      "Scene 1352890838234: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838234 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829166: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829166 contains -1 edges. Removing these edges.\n",
      "Scene 1352890845672: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890845672 contains -1 edges. Removing these edges.\n",
      "NOTE! Scene 1352890845672 skipped: no valid nodes after filtering.\n",
      "Scene 1352890842712: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890842712 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832497: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832497 contains -1 edges. Removing these edges.\n",
      "Scene 1352890843708: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890843708 contains -1 edges. Removing these edges.\n",
      "Scene 13528908058: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908058 contains -1 edges. Removing these edges.\n",
      "Scene 1352890839439 contains -1 edges. Removing these edges.\n",
      "Scene 13528908003660002: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908003660002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824946 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824516 contains -1 edges. Removing these edges.\n",
      "Scene 135289081098 contains -1 edges. Removing these edges.\n",
      "Scene 13528908292589998: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908292589998 contains -1 edges. Removing these edges.\n",
      "Scene 1352890817934: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890817934 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800907 contains -1 edges. Removing these edges.\n",
      "Scene 1352890815016: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890815016 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834692 contains -1 edges. Removing these edges.\n",
      "Scene 1352890867565: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890867565 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814839: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890814839 contains -1 edges. Removing these edges.\n",
      "Scene 1352890916428: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890916428 contains -1 edges. Removing these edges.\n",
      "Scene 1352890887794: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890887794 contains -1 edges. Removing these edges.\n",
      "Scene 1352890813762 contains -1 edges. Removing these edges.\n",
      "Scene 13528908331360002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807035: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890807035 contains -1 edges. Removing these edges.\n",
      "Scene 1352890807396: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890807396 contains -1 edges. Removing these edges.\n",
      "Scene 1352890806949: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890806949 contains -1 edges. Removing these edges.\n",
      "Scene 1352890801417: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890801417 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809687: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809687 contains -1 edges. Removing these edges.\n",
      "Scene 1352890820225 contains -1 edges. Removing these edges.\n",
      "Scene 135289080722: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080722 contains -1 edges. Removing these edges.\n",
      "Scene 1352890891769: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890891769 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832459: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832459 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894309: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890894309 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837646 contains -1 edges. Removing these edges.\n",
      "Scene 1352890829843: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890829843 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841153: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890841153 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800768: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800768 contains -1 edges. Removing these edges.\n",
      "Scene 1352890811843: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890811843 contains -1 edges. Removing these edges.\n",
      "Scene 1352890827838 contains -1 edges. Removing these edges.\n",
      "Scene 13528908329: Filtering 1 nodes with NaN values.\n",
      "Scene 13528908329 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803659: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890803659 contains -1 edges. Removing these edges.\n",
      "Scene 135289083891 contains -1 edges. Removing these edges.\n",
      "Scene 1352890804018: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890804018 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814167: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890814167 contains -1 edges. Removing these edges.\n",
      "Scene 1352890875117 contains -1 edges. Removing these edges.\n",
      "Scene 1352890836572 contains -1 edges. Removing these edges.\n",
      "Scene 1352890825035 contains -1 edges. Removing these edges.\n",
      "Scene 135289083596: Filtering 1 nodes with NaN values.\n",
      "Scene 135289083596 contains -1 edges. Removing these edges.\n",
      "Scene 13528908282510002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802778: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890802778 contains -1 edges. Removing these edges.\n",
      "Scene 1352890812416: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890812416 contains -1 edges. Removing these edges.\n",
      "Scene 135289083913 contains -1 edges. Removing these edges.\n",
      "Scene 1352890832378: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890832378 contains -1 edges. Removing these edges.\n",
      "Scene 1352890917968: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890917968 contains -1 edges. Removing these edges.\n",
      "Scene 13528908366160002 contains -1 edges. Removing these edges.\n",
      "Scene 1352890823862: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890823862 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919792: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919792 contains -1 edges. Removing these edges.\n",
      "Scene 1352890872601: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890872601 contains -1 edges. Removing these edges.\n",
      "Scene 1352890894888: Filtering 4 nodes with NaN values.\n",
      "Scene 1352890894888 contains -1 edges. Removing these edges.\n",
      "Scene 135289091426 contains -1 edges. Removing these edges.\n",
      "Scene 1352890919969: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890919969 contains -1 edges. Removing these edges.\n",
      "Scene 1352890835036: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890835036 contains -1 edges. Removing these edges.\n",
      "Scene 1352890809912: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890809912 contains -1 edges. Removing these edges.\n",
      "Scene 135289080146: Filtering 1 nodes with NaN values.\n",
      "Scene 135289080146 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833505 contains -1 edges. Removing these edges.\n",
      "Scene 1352890895393: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890895393 contains -1 edges. Removing these edges.\n",
      "Scene 1352890838867: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838867 contains -1 edges. Removing these edges.\n",
      "Scene 1352890833338 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819768: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890819768 contains -1 edges. Removing these edges.\n",
      "Scene 1352890803216: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890803216 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808433 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800052: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800052 contains -1 edges. Removing these edges.\n",
      "Scene 1352890811475: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890811475 contains -1 edges. Removing these edges.\n",
      "Scene 1352890834655 contains -1 edges. Removing these edges.\n",
      "Scene 1352890814706: Filtering 2 nodes with NaN values.\n",
      "Scene 1352890814706 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837601 contains -1 edges. Removing these edges.\n",
      "Scene 1352890808346 contains -1 edges. Removing these edges.\n",
      "Scene 1352890818385: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890818385 contains -1 edges. Removing these edges.\n",
      "Scene 1352890838146: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890838146 contains -1 edges. Removing these edges.\n",
      "Scene 1352890800142: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890800142 contains -1 edges. Removing these edges.\n",
      "Scene 1352890851846: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890851846 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837115 contains -1 edges. Removing these edges.\n",
      "Scene 13528909148 contains -1 edges. Removing these edges.\n",
      "Scene 1352890908102: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890908102 contains -1 edges. Removing these edges.\n",
      "Scene 1352890841295 contains -1 edges. Removing these edges.\n",
      "Scene 1352890870132: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890870132 contains -1 edges. Removing these edges.\n",
      "Scene 1352890837204 contains -1 edges. Removing these edges.\n",
      "Scene 1352890802191 contains -1 edges. Removing these edges.\n",
      "Scene 1352890819948 contains -1 edges. Removing these edges.\n",
      "Scene 1352890824206 contains -1 edges. Removing these edges.\n",
      "Scene 135289081789: Filtering 1 nodes with NaN values.\n",
      "Scene 135289081789 contains -1 edges. Removing these edges.\n",
      "Scene 1352890910955: Filtering 1 nodes with NaN values.\n",
      "Scene 1352890910955 contains -1 edges. Removing these edges.\n",
      "Loaded 189 scenes.\n",
      "Train scenes: 132, Val scenes: 28, Test scenes: 29\n",
      "\n",
      "Running Task 3...\n",
      "\n",
      "Training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling CosineGraphAttentionNetwork.call().\n\n\u001b[1mInput 0 of layer \"dense_14\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\u001b[0m\n\nArguments received by CosineGraphAttentionNetwork.call():\n  • inputs=tf.Tensor(shape=(None, 4), dtype=float32)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m         gat_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_and_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgat_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgat_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m         evaluate_and_plot(gat_model\u001b[38;5;241m=\u001b[39mgat_model, history\u001b[38;5;241m=\u001b[39mhistory, test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset, task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m    112\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[5], line 144\u001b[0m, in \u001b[0;36mcompile_and_train\u001b[0;34m(gat_model, train_dataset, val_dataset, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    139\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m    140\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgat_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gat_model, history\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[5], line 412\u001b[0m, in \u001b[0;36mCosineGraphAttentionNetwork.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    410\u001b[0m node_features \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    411\u001b[0m edges \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 412\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layers:\n\u001b[1;32m    414\u001b[0m     x_new \u001b[38;5;241m=\u001b[39m attn_layer([x, edges])\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling CosineGraphAttentionNetwork.call().\n\n\u001b[1mInput 0 of layer \"dense_14\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (4,)\u001b[0m\n\nArguments received by CosineGraphAttentionNetwork.call():\n  • inputs=tf.Tensor(shape=(None, 4), dtype=float32)\n  • training=True"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dataset_dir = \"dataset\"\n",
    "\n",
    "# tasks = [1, 2, 3]\n",
    "tasks = [3]\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    feature_cols = [\"current_x\", \"current_y\", \"previous_x\", \"previous_y\"]\n",
    "    target_cols = [\"future_x\", \"future_y\"]\n",
    "\n",
    "    scenes = load_all_subgraphs(dataset_dir)\n",
    "    print(f\"Loaded {len(scenes)} scenes.\")\n",
    "    train_scenes, val_scenes, test_scenes = split_scenes(scenes, train_ratio=0.7, val_ratio=0.15)\n",
    "    print(f\"Train scenes: {len(train_scenes)}, Val scenes: {len(val_scenes)}, Test scenes: {len(test_scenes)}\")\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(train_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(val_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: scene_generator(test_scenes, feature_cols, target_cols),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(None, len(target_cols)), dtype=tf.float32),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(100).batch(1).map(squeeze_batch)\n",
    "    val_dataset = val_dataset.batch(1).map(squeeze_batch)\n",
    "    test_dataset = test_dataset.batch(1).map(squeeze_batch)\n",
    "\n",
    "    HIDDEN_UNITS = 100\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 3\n",
    "    OUTPUT_DIM = 2\n",
    "    LEARNING_RATE = 1e-2\n",
    "    NUM_EPOCHS = 100\n",
    "\n",
    "    gat_model = None\n",
    "    history = None\n",
    "\n",
    "    if task == 1:\n",
    "        print(\"\\nRunning Task 1...\\n\")\n",
    "\n",
    "        gat_model = GraphAttentionNetwork(\n",
    "            hidden_units=HIDDEN_UNITS, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, output_dim=OUTPUT_DIM, task=task\n",
    "        )\n",
    "\n",
    "    elif task == 2:\n",
    "        print(\"\\nRunning Task 2...\\n\")\n",
    "        num_heads = [4, 8, 16]\n",
    "\n",
    "        for i, heads in enumerate(num_heads):\n",
    "\n",
    "            gat_model = GraphAttentionNetwork(\n",
    "                hidden_units=HIDDEN_UNITS, num_heads=heads, num_layers=NUM_LAYERS, output_dim=OUTPUT_DIM, task=task\n",
    "            )\n",
    "\n",
    "            gat_model, history = compile_and_train(\n",
    "                gat_model=gat_model,\n",
    "                train_dataset=train_dataset,\n",
    "                val_dataset=val_dataset,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "            )\n",
    "\n",
    "            evaluate_and_plot(gat_model=gat_model, history=history, test_dataset=test_dataset, task=task)\n",
    "\n",
    "    elif task == 3:\n",
    "        print(\"\\nRunning Task 3...\\n\")\n",
    "\n",
    "        gat_model = CosineGraphAttentionNetwork(\n",
    "            hidden_units=HIDDEN_UNITS,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            output_dim=OUTPUT_DIM,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown task\")\n",
    "    \n",
    "    if task != 2:\n",
    "        gat_model, history = compile_and_train(\n",
    "            gat_model=gat_model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "    \n",
    "        evaluate_and_plot(gat_model=gat_model, history=history, test_dataset=test_dataset, task=task)\n",
    "\n",
    "end_time = time.time()\n",
    "running_time = end_time - start_time\n",
    "hours = int(running_time // 3600)\n",
    "minutes = int((running_time % 3600) // 60)\n",
    "seconds = int(running_time % 60)\n",
    "print(f\"Running time: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
